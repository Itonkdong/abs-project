_wandb:
    value:
        cli_version: 0.23.1
        e:
            9e49q90heo5qlx2fgh58wb8tm5pxv5n8:
                codePath: main_andrea.ipynb
                codePathLocal: main_andrea.ipynb
                colab: https://colab.research.google.com/notebook#fileId=https://github.com/Itonkdong/abs-project/blob/main/src/experiments/andrea/main_andrea.ipynb
                cpu_count: 1
                cpu_count_logical: 2
                cudaVersion: "12.4"
                disk:
                    /:
                        total: "120942624768"
                        used: "49406373888"
                executable: /usr/bin/python3
                git:
                    commit: 7d4e45ccf3cc41b1e430102ea06440b1b198de74
                    remote: https://Andrea-444:@github.com/itonkdong/abs-project.git
                gpu: Tesla T4
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Turing
                      cudaCores: 2560
                      memoryTotal: "16106127360"
                      name: Tesla T4
                      uuid: GPU-7ec4a17e-383a-fdb8-7fe2-eaf5eb191557
                host: 79dbc2ecd068
                memory:
                    total: "13605851136"
                os: Linux-6.6.105+-x86_64-with-glibc2.35
                program: main_andrea.ipynb
                python: CPython 3.12.12
                root: /content/abs-project/src/experiments/andrea/masac_multi_give_way_mlp__7e255231_26_01_13-15_30_44
                startedAt: "2026-01-13T15:30:51.064288Z"
                writerId: 9e49q90heo5qlx2fgh58wb8tm5pxv5n8
        m: []
        python_version: 3.12.12
        t:
            "1":
                - 1
                - 41
                - 50
                - 105
            "2":
                - 1
                - 41
                - 50
                - 105
            "3":
                - 2
                - 3
                - 13
                - 14
                - 16
            "4": 3.12.12
            "5": 0.23.1
            "8":
                - 1
                - 12
            "12": 0.23.1
            "13": linux-x86_64
algorithm_config:
    value:
        alpha_init: 0.2
        coupled_discrete_values: true
        delay_qvalue: true
        discrete_target_entropy_weight: 0.2
        fixed_alpha: false
        loss_function: l2
        max_alpha: null
        min_alpha: null
        num_qvalue_nets: 2
        scale_mapping: biased_softplus_2.0
        share_param_critic: false
        target_entropy: auto
        use_tanh_normal: true
algorithm_name:
    value: masac
continuous_actions:
    value: true
critic_model_config:
    value:
        _is_critic: true
        activation_class: torch.nn.modules.activation.Tanh
        activation_kwargs: null
        layer_class: torch.nn.modules.linear.Linear
        norm_class: null
        norm_kwargs: null
        num_cells:
            - 256
            - 256
        num_feature_dims: 1
critic_model_name:
    value: mlp
environment_name:
    value: vmas
experiment_config:
    value:
        adam_eps: 1e-06
        buffer_device: cpu
        checkpoint_at_end: false
        checkpoint_interval: 0
        clip_grad_norm: true
        clip_grad_val: 5
        collect_with_grad: false
        create_json: true
        evaluation: true
        evaluation_deterministic_actions: true
        evaluation_episodes: 10
        evaluation_interval: 120000
        evaluation_static: false
        exclude_buffer_from_checkpoint: false
        exploration_anneal_frames: null
        exploration_eps_end: 0.01
        exploration_eps_init: 0.8
        gamma: 0.99
        hard_target_update_frequency: 5
        keep_checkpoints_num: 3
        loggers:
            - csv
            - wandb
        lr: 5e-05
        max_n_frames: 1200000
        max_n_iters: null
        off_policy_collected_frames_per_batch: 6000
        off_policy_init_random_frames: 0
        off_policy_memory_size: 1000000
        off_policy_n_envs_per_worker: 10
        off_policy_n_optimizer_steps: 1000
        off_policy_prb_alpha: 0.6
        off_policy_prb_beta: 0.4
        off_policy_train_batch_size: 128
        off_policy_use_prioritized_replay_buffer: false
        on_policy_collected_frames_per_batch: 6000
        on_policy_minibatch_size: 400
        on_policy_n_envs_per_worker: 10
        on_policy_n_minibatch_iters: 45
        parallel_collection: false
        polyak_tau: 0.005
        prefer_continuous_actions: true
        project_name: benchmarl
        render: true
        restore_file: null
        restore_map_location: null
        sampling_device: cpu
        save_folder: null
        share_policy_params: true
        soft_target_update: true
        train_device: cuda
model_config:
    value:
        _is_critic: false
        activation_class: torch.nn.modules.activation.Tanh
        activation_kwargs: null
        layer_class: torch.nn.modules.linear.Linear
        norm_class: null
        norm_kwargs: null
        num_cells:
            - 256
            - 256
        num_feature_dims: 1
model_name:
    value: mlp
on_policy:
    value: false
seed:
    value: 0
task_config:
    value:
        agent_collision_penalty: -0.1
        max_steps: 250
task_name:
    value: multi_give_way
